# PwnAI Model Configuration
# Define different model providers and configurations

openai:
  url: https://api.openai.com/v1
  model: gpt-4o
  temperature: 0.2
  system_prompt_prefix: "You are a binary exploitation expert assistant helping solve CTF challenges."

ollama:
  url: http://host.docker.internal:11434/api/chat
  model: qwq:32b
  temperature: 0.2
  num_ctx: 16384
  system_prompt_prefix: "You are a binary exploitation expert assistant helping solve CTF challenges."

ollama_small:
  url: http://host.docker.internal:11434/api/chat
  model: llama3.2:3b
  temperature: 0.2
  num_ctx: 16384
  system_prompt_prefix: "You are a binary exploitation expert assistant helping solve CTF challenges."

gemma3:
  url: http://host.docker.internal:11434/api/chat
  model: gemma3:27b
  temperature: 0.2
  num_ctx: 16384
  system_prompt_prefix: "You are a binary exploitation expert assistant helping solve CTF challenges."

claude:
  url: https://api.anthropic.com/v1/messages
  model: claude-3-7-sonnet-20250219
  temperature: 0.2
  system_prompt_prefix: "You are a binary exploitation expert assistant helping solve CTF challenges." 
  # Uncomment and set your Anthropic API key here
  # api_key: your_api_key_here
  default: true
